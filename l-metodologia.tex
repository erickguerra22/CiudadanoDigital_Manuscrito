El desarrollo del proyecto \textit{Ciudadano Digital} se llevó a cabo bajo el
marco de trabajo SCRUM, un enfoque ágil que permite la entrega incremental de
productos funcionales mediante ciclos cortos de desarrollo (\textit{sprints}).
Esta metodología fue seleccionada debido a su flexibilidad, capacidad de
adaptación a cambios en los requerimientos y enfoque en la mejora continua,
elementos clave en un proyecto de innovación educativa.

A lo largo del proceso, se definieron seis \textit{sprints} principales, cada
uno orientado a la obtención progresiva de un prototipo funcional y validado de
la aplicación (Cuadro \ref{tab:estructura-sprints}). Cada \textit{sprint} tuvo
una duración de entre tres y cuatro semanas, ajustándose según la complejidad
técnica y la carga académica del periodo correspondiente.

\begin{table}[H]
      \centering
      \renewcommand{\arraystretch}{1.2}
      \begin{tabular}{|l|p{8cm}|c|}
            \hline
            \textbf{\textit{Sprint}} & \textbf{Meta Principal}                                  & \textbf{Duración} \\ \hline
            \textit{Sprint} 1        & Identificación del perfil de usuario objetivo (Persona). & 3 semanas         \\ \hline
            \textit{Sprint} 2        & Recolección y procesamiento del contenido educativo.     & 4 semanas         \\ \hline
            \textit{Sprint} 3        & Construcción e implementación del servidor.              & 4 semanas         \\ \hline
            \textit{Sprint} 4        & Desarrollo de la interfaz móvil en Kotlin.               & 4 semanas         \\ \hline
            \textit{Sprint} 5        & Pruebas y validación funcional.                          & 3 semanas         \\ \hline
            \textit{Sprint} 6        & Documentación, presentación y cierre del proyecto.       & 3 semanas         \\ \hline
      \end{tabular}
      \caption[Estructura de los \textit{Sprints}]{Estructura de los \textit{sprints} del proyecto, incluyendo la meta principal y duración de cada uno.}
      \label{tab:estructura-sprints}
\end{table}

Cada ciclo SCRUM siguió las fases de planificación, desarrollo, revisión y
retrospectiva, bajo los siguientes principios:

\begin{itemize}
      \item \textbf{Planificación (Planeación del Sprint):} se definieron los objetivos y alcance
            del \textit{sprint}, así como las tareas específicas necesarias para cumplir la
            meta establecida.
      \item \textbf{Desarrollo (Ejecución del Sprint):} se ejecutaron las tareas asignadas con
            enfoque en la funcionalidad incremental, priorizando siempre la obtención de
            resultados medibles.
      \item \textbf{Revisión (Revisión del Sprint):} al cierre de cada \textit{sprint}, se evaluó el
            cumplimiento de los objetivos, la calidad del producto obtenido y la
            satisfacción de los criterios de aceptación definidos.
      \item \textbf{Retrospectiva (\textit{Sprint Retrospective}):} se analizaron los aprendizajes
            obtenidos, los obstáculos encontrados y las oportunidades de mejora para el
            siguiente \textit{sprint}.
\end{itemize}

El producto mínimo viable (MVP, por sus siglas en inglés) obtenido al finalizar
el último \textit{sprint} constituye una versión funcional del asistente
inteligente de educación ciudadana, capaz de interactuar con el usuario,
contextualizar sus preguntas y generar respuestas basadas en la información
previamente curada y vectorizada.

\section{\textit{Sprint} 1: Identificación del perfil de usuario objetivo}
\textbf{Duración:} 3 semanas

Este \textit{sprint} tuvo como objetivo desarrollar un perfil de usuario
(Persona) que sirviera como insumo accionable para orientar las decisiones de
diseño interactivo y priorización técnica del proyecto. Dado que no fue posible
realizar entrevistas ni trabajo de campo, el perfil se elaboró exclusivamente a
partir del análisis de fuentes documentales que reflejan la situación actual de
los estudiantes en el país, considerando aspectos demográficos, académicos y
sociales. Con base en esta información, se construyó una ficha de
\textbf{Persona} completa, acompañada de criterios de diseño alineados con las
necesidades y características identificadas.

\subsection{Ejecución}
La culminación del \textit{sprint} se evaluó tomando en cuenta la culminación
exitosa de las siguientes tareas:

\begin{enumerate}
      \item \textbf{Investigación documental}
            \begin{itemize}
                  \item Revisión de informes académicos y/o gubernamentales sobre educación ciudadana,
                        competencias cívicas y valores en jóvenes guatemaltecos.
                  \item Consulta de programas educativos oficiales, como el Currículo Nacional Base
                        (CNB) y materiales de formación en valores del Ministerio de Educación de
                        Guatemala, así como contenido internacional enfocado en brindar una educación
                        más completa \cite{mineduc2020cnb,mineduc2019guia,Toro2010-iq}.
                  \item Análisis de estudios internacionales de organismos como UNESCO (Organización de
                        las Naciones Unidas para la Educación, la Ciencia y la Cultura) y CIEN (Centro
                        de Investigaciones Económicas Nacionales) sobre hábitos digitales, desigualdad
                        educativa y desarrollo de competencias ciudadanas en adolescentes y jóvenes
                        \cite{unesco2023monitoring,unesco2021reimagining,cien2019diagnostico}.
            \end{itemize}

      \item \textbf{Análisis e interpretación de la información}
            \begin{itemize}
                  \item Sistematización de datos demográficos, educativos y tecnológicos relevantes
                        para el contexto juvenil guatemalteco.
                  \item Identificación de patrones generales de comportamiento, motivaciones,
                        frustraciones y objetivos (enfocados en aspiraciones cívicas), a partir de
                        tendencias reportadas en las fuentes analizadas.
                  \item Construcción de categorías de análisis que permitieran traducir los hallazgos
                        documentales en insumos para el diseño centrado en el usuario.
            \end{itemize}

      \item \textbf{Definición del perfil Persona}
            \begin{itemize}
                  \item Elaboración de una ficha de usuario basada en la interpretación crítica de los
                        datos documentales, con los siguientes componentes:
                        \begin{itemize}
                              \item \textbf{Perfil base:} edad estimada, nivel educativo, ubicación, etnia, acceso tecnológico y contexto social.
                              \item \textbf{Motivaciones:} interés por la participación comunitaria y el aprendizaje de ciudadanía.
                              \item \textbf{Frustraciones:} barreras de acceso a recursos educativos y desconfianza en la calidad o adecuación de los materiales disponibles.
                              \item \textbf{Objetivos:} las metas que el usuario quisiera conseguir a través de sus motivaciones y frustraciones, bajo el contexto de educación en valores y formación ciudadana.
                              \item \textbf{Consideraciones especiales:} limitaciones de conectividad, recursos económicos y brechas culturales.
                        \end{itemize}
                  \item Producción de una ficha visual que sirviera como base para las decisiones de
                        diseño en \textit{sprints} posteriores.
            \end{itemize}

      \item \textbf{Documentación de criterios de diseño}
            \begin{itemize}
                  \item Derivación de recomendaciones de diseño UX basadas en el perfil construido:
                        tono comunicativo, estructura de funciones, rol a asumir por el asistente, y
                        adaptabilidad tecnológica.
                  \item Identificación de necesidades prioritarias que el asistente debe ser capaz de
                        abordar a través de la interacción pregunta-respuesta.
            \end{itemize}

\end{enumerate}

\section{\textit{Sprint} 2: Recolección y procesamiento del contenido educativo inicial}
\textbf{Duración:} 4 semanas

Este \textit{sprint} se centró en recopilar, procesar y estructurar el
contenido educativo inicial que alimenta inicialmente al asistente virtual de
inteligencia artificial. El fin de este proceso fue garantizar que el sistema
puediera generar respuestas precisas y contextualizadas sobre formación
ciudadana y valores morales, basándose en información confiable y organizada de
manera semántica.

\subsection{Ejecución}

Se desarrolló un proceso sistemático dividido en cuatro etapas principales:
selección documental, curación, segmentación temática, y vectorización (a
través de OpenAI) con almacenamiento en Pinecone. Este flujo se diseñó de forma
reproducible para permitir futuras ampliaciones o actualizaciones del corpus de
información.

\begin{enumerate}

      \item \textbf{Selección documental}
            \begin{itemize}
                  \item \textbf{Identificación de fuentes oficiales y confiables:} se recopilaron documentos emitidos por el Ministerio de Educación de Guatemala, tales como contenidos contemplados en el \textbf{Currículo Nacional Base (CNB)} para los grados educativos abarcados por el rango de edad establecido. También se recolectaron guías orientacionales dirigidas a los educadores, con el fin de que el asistente también tenga conocimiento de cómo interactuar con los usuarios objetivo de forma correcta.
                  \item \textbf{Revisión de fuentes internacionales:} se incorporaron publicaciones y estudios de entidades internacionales como la OEA (Organización de Estados Americanos), la UNESCO (Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura) o universidades extranjeras. Mediante este contenido, se buscó alimentar aún más el conocimiento teórico del asistente, así como diversificar las fuentes de información a contextos internacionales.
                  \item \textbf{Estudios complementarios:} Además de las fuentes mencionadas, también se incluyó la recopilación de libros educativos de entidades independientes (tales como IGER, Instituto Guatemalteco de Educación Radifónica) así como de autores externos. \cite{iger2024cienciasciudadana}
                  \item \textbf{Registro de metadatos:} cada documento fue almacenado en un contenedor tipo \textbf{Amazon S3} (\textit{Simple Storage Service}). Los metadatos asociados a cada documento (título, autor, año de publicación), así como la ruta de almacenamiento relativa dentro del contenedor, fueron almacenados en la base de datos vectorial, según el siguiente esquema:
                        \begin{itemize}
                              \item Identificador único (\texttt{DocumentID})
                              \item Identificador del usuario que sube el documento (\texttt{UserID})
                              \item Título del documento (\texttt{Title})
                              \item Fuente o Autor (\texttt{Author})
                              \item Año de publicación (\texttt{Year})
                              \item Categoría temática (\texttt{Category})
                              \item Ruta dentro del contenedor S3 (\texttt{Document URL})
                        \end{itemize}
                        Este registro garantiza la trazabilidad desde la fuente original hasta el fragmento vectorizado. Cabe aclarar que el uso de variables en inglés corresponde a buenas prácticas de programación, con el fin de que el código pueda ser comprendido a nivel global en caso de ser necesario \cite{batubara2025impact}.
            \end{itemize}

      \item \textbf{Curación y digitalización}
            \begin{itemize}
                  \item \textbf{Conversión de documentos:} los archivos se transformaron a texto plano (\texttt{.txt}) con codificación UTF-8 mediante herramientas como \texttt{NFKD} (siglas en inglés para Descomposición de Compatibilidad de Formas de Normalización) o \texttt{Tesseract OCR}. Esta última extrae de forma automática el texto reconocible de imágenes o documentos PDF escaneados (Reconocimiento Óptico de Caracteres).
                  \item \textbf{Limpieza y normalización:} se eliminaron saltos de línea innecesarios, espacios vacíos múltiples y caracteres especiales, a través del Algoritmo \ref{alg:limpiar-texto}.

                        \begin{algorithm}[H]
                              \caption{Proceso de limpieza y normalización profunda de texto}
                              \label{alg:limpiar-texto}
                              \begin{algorithmic}[1]
                                    \Procedure{LimpiarTexto}{texto}
                                    \State texto $\gets$ NormalizarUnicode(texto, \guillemetleft{}NFKD\guillemetright{})
                                    \State texto $\gets$ EliminarCaracteresNoASCII(texto)
                                    \State texto $\gets$ Reemplazar(texto, \{\guillemetleft{}\textbackslash r\guillemetright{}, \guillemetleft{}\textbackslash n\guillemetright{}, \guillemetleft{}\textbackslash t\guillemetright{}\}, \guillemetleft{} \guillemetright{})
                                    \State texto $\gets$ EliminarCaracteresEspeciales(texto, \guillemetleft{}manteniendo letras, números y puntuación básica\guillemetright{})
                                    \State texto $\gets$ ReemplazarMúltiplesEspaciosPorUno(texto)
                                    \State texto $\gets$ EliminarEspaciosExtremos(texto)
                                    \State \Return texto
                                    \EndProcedure
                              \end{algorithmic}
                        \end{algorithm}

                  \item \textbf{Estandarización de formato:} se uniformaron títulos y subtítulos con reglas jerárquicas para facilitar la segmentación automática, como se muestra en el Algoritmo \ref{alg:estandarizar-texto}.
                        \begin{algorithm}[H]
                              \caption{Estandarización de títulos y numeración en texto}
                              \label{alg:estandarizar-texto}
                              \begin{algorithmic}[1]
                                    \Procedure{EstandarizarFormato}{texto}
                                    \State texto $\gets$ ReemplazarMarkdownConTitulo(texto)
                                    \State texto $\gets$ ReemplazarNumeracionConTitulo(texto)
                                    \State texto $\gets$ ConvertirTitulosMayusculas(texto)
                                    \State texto $\gets$ UniformarNumeracion(texto)
                                    \State texto $\gets$ EliminarEspaciosExtremos(texto)
                                    \State \Return texto
                                    \EndProcedure
                              \end{algorithmic}
                        \end{algorithm}

                  \item \textbf{Validación de integridad:} se verificó que los textos conservaran coherencia y completitud al eliminar duplicados o secciones ilegibles. Se siguió el flujo descrito en el Algoritmo \ref{alg:validar-integridad}.

                        \begin{algorithm}[H]
                              \caption{Validación de integridad de texto}
                              \label{alg:validar-integridad}
                              \begin{algorithmic}[1]
                                    \Procedure{ValidarIntegridad}{texto}
                                    \State líneas $\gets$ DividirEnLineas(texto)
                                    \State líneas\_limpias $\gets$ ListaVacía()
                                    \For{cada línea en líneas}
                                    \If{Longitud(Trim(linea)) < 3}
                                    \State Continuar
                                    \EndIf
                                    \State caracteres\_válidos $\gets$ ContarCaracteresAlfanumericosYEspacios(linea)
                                    \If{caracteres\_válidos / Max(Longitud(linea), 1) > 0.6}
                                    \State Añadir(linea, líneas\_limpias)
                                    \EndIf
                                    \EndFor
                                    \State líneas\_sin\_duplicados $\gets$ EliminarDuplicados(líneas\_limpias)
                                    \State texto\_limpio $\gets$ UnirLineas(líneas\_sin\_duplicados)
                                    \State \Return texto\_limpio
                                    \EndProcedure
                              \end{algorithmic}
                        \end{algorithm}
            \end{itemize}

      \item \textbf{Segmentación temática}
            \begin{itemize}
                  \item \textbf{Diseño del esquema de categorías:} se definieron seis temas guía iniciales: \textit{ética y moral}, \textit{participación ciudadana}, \textit{derechos humanos}, \textit{convivencia y respeto}, \textit{responsabilidad social} y \textit{cultura digital}. Esta lista puede incrementarse con el tiempo, a medida que el modelo procese una mayor cantidad de archivos y no sea capaz de incluirlos en una de las categorías predefinidas.
                  \item \textbf{División en fragmentos:} los textos fueron segmentados automáticamente en bloques de 20 a 150 palabras. Se respetaron signos de puntuación, saltos de línea, listas y numeración de títulos con el fin de preservar la coherencia semántica.
                  \item \textbf{Etiquetado y registro:} cada fragmento se asoció a una categoría temática y se describió con los siguientes metadatos:
                        \texttt{document\_id} (identificador único del documento en la base de datos relacional), \texttt{text} (contenido original del documento), \texttt{source} (título original del documento), \texttt{author} (autor o institución que realizó el documento), \texttt{year} (año de publicación del documento original), \texttt{category} (categoría temática asociada al fragmento), \texttt{sha1} (\textit{hash} único del fragmento, para evitar duplicados), \texttt{uploaded\_at} (fecha y hora de publicación del fragmento, en formato ISO).
            \end{itemize}

      \item \textbf{Vectorización y almacenamiento en Pinecone}
            \begin{itemize}
                  \item \textbf{Generación de representaciones numéricas:} cada fragmento fue procesado con el modelo \textit{text-embedding-3-small} de OpenAI, lo que permitió generar vectores de 1536 dimensiones.
                  \item \textbf{Normalización final:} la asignación del metadato \textit{sha1} en cada vector permitió validar la ausencia de duplicados, lo que asegura la unicidad de cada vector en la base de datos vectorial.
                  \item \textbf{Creación del índice vectorial:} se configuró un índice en Pinecone con los parámetros:
                        \begin{itemize}
                              \item \texttt{name = \guillemetleft{}ciudadano-digital\guillemetright{}}
                              \item \texttt{namespace = \guillemetleft{}ciudadania\guillemetright{}}
                              \item \texttt{metric = \guillemetleft{}cosine\guillemetright{}}
                              \item \texttt{dimension = 1536}
                        \end{itemize}
                  \item \textbf{Inserción de vectores:} la representación numérica de cada fragmento se insertó junto con sus metadatos para permitir consultas semánticas eficientes. El proceso completo se muestra en el Algoritmo \ref{alg:vectorizar-fragmentos}. Destaca la presencia de la variable \textbf{BATCH\_SIZE}, la cual corresponde a una constante utilizada para definir el tamaño del paquete de vectores enviado a la base de datos. Esto busca evitar enviar vectores individuales que podrían ralentizar el proceso de almacenamiento y elevar los costos relacionados a la base de datos vectorial.

                        \begin{algorithm}[H]
                              \caption{Vectorización de fragmentos}
                              \label{alg:vectorizar-fragmentos}
                              \begin{algorithmic}[1]
                                    \Procedure{VectorizarFragmentos}{fragmentos, identificador, fuente, autor, año, BATCH\_SIZE}
                                    \State lote $\gets$ ListaVacía()
                                    \For{cada frag en fragmentos}
                                    \State sha1 $\gets$ CalcularSHA1(frag)
                                    \If{FragmentoYaIndexado(sha1) \textbf{or} EsVacio(frag)}
                                    \State Continuar
                                    \EndIf
                                    \State categoria $\gets$ ClasificarCategoria(frag)
                                    \State embedding $\gets$ GenerarEmbedding(modelo=\guillemetleft{}text-embedding-3-small\guillemetright{}, texto=frag)
                                    \State metadatos $\gets$ CrearDiccionario(\{
                                    \guillemetleft{}document\_id\guillemetright{}: identificador,
                                    \guillemetleft{}text\guillemetright{}: frag,
                                    \guillemetleft{}source\guillemetright{}: fuente,
                                    \guillemetleft{}author\guillemetright{}: autor,
                                    \guillemetleft{}year\guillemetright{}: año,
                                    \guillemetleft{}category\guillemetright{}: categoria,
                                    \guillemetleft{}sha1\guillemetright{}: sha1,
                                    \guillemetleft{}uploaded\_at\guillemetright{}: FechaHoraActual()
                                    \})
                                    \State AñadirAlLote(lote, CrearVector(id=GenerarUUID(), valores=embedding, metadatos=metadatos))
                                    \If{Longitud(lote) $\ge$ BATCH\_SIZE}
                                    \State RegistrarLote(lote, namespace=\guillemetleft{}ciudadanía\guillemetright{})
                                    \State lote $\gets$ ListaVacía()
                                    \EndIf
                                    \EndFor
                                    \If{lote \textbf{no vacío}}
                                    \State RegistrarLote(lote)
                                    \EndIf
                                    \EndProcedure
                              \end{algorithmic}
                        \end{algorithm}

                  \item \textbf{Implementación de flujo para procesamiento de archivos}
                        El sistema obtenido a través del proceso anterior, permitió procesar
                        diversos formatos de archivo:
                        \begin{itemize}
                              \item Documentos PDF (.pdf)
                              \item Documentos de Word (.doc, .docx)
                              \item Presentaciones (.ppt, .pptx)
                              \item Texto plano (.txt)
                              \item Markdown (.md)
                              \item Imágenes y documentos escaneados (mediante Tesseract OCR)
                        \end{itemize}
                        La Figura \ref{fig:procesamiento-docs} ilustra el flujo completo de
                        procesamiento:

                        \begin{figure}[H]
                              \centering
                              \includegraphics[width=0.9\textwidth]{assets/procesamiento-docs.png}
                              \caption{Flujo de procesamiento de documentos para generación del corpus.}
                              \label{fig:procesamiento-docs}
                        \end{figure}

                        El proceso se dividió en dos etapas principales:

                        \begin{enumerate}
                              \item \textbf{Extracción y normalización del contenido:} En esta etapa se realizó la lectura y extracción de texto desde los formatos soportados. El flujo incluyó la limpieza del texto, la estandarización del formato (identificación de títulos, encabezados y listas para preservar la semántica), la validación de la integridad (omisión de duplicados, normalización de caracteres) y la fragmentación semántica. El producto final resultante está compuesto por fragmentos de texto plano autocontenidos.
                              \item \textbf{Generación de representaciones numéricas e indexación vectorial:} Cada fragmento de texto se transformó en un vector mediante el modelo de OpenAI \texttt{text-embedding-3-small}. A cada representación numérica de texto, se le asociaron metadatos relevantes (identificador del documento original, título, categoría temática, texto original, autor y año de publicación). Posteriormente, se realiza una operación de \textbf{\textit{upsert}} (es decir, insertar o actualizar el registro según corresponda) en el índice vectorial de Pinecone. Simultáneamente, a través del identificador del documento original, se registró en la base de datos relacional la trazabilidad entre el vector y el archivo almacenado en un contenedor de Amazon S3.
                        \end{enumerate}
            \end{itemize}
\end{enumerate}

\section{\textit{Sprint} 3: Construcción e implementación del servidor}
\textbf{Duración:} 4 semanas

Este \textit{sprint} se enfocó en el diseño, construcción e implementación de
la arquitectura del servidor del asistente virtual. Se buscó garantizar la
integración de bases de datos relacionales y vectoriales, así como establecer
la comunicación segura y eficiente con el modelo grande de lenguaje (LLM, por
sus siglas en inglés) mediante un flujo de generación mejorada por
recuperación. Se definieron módulos claros bajo el patrón de diseño MVC
(Modelo-Vista-Controlador), así como servicios complementarios internos en
Python tanto para la curación y procesamiento de documentos, como para procesar
consultas y generar respuestas contextualizadas con base en los mismos.

\subsection{Ejecución}
\begin{enumerate}
      \item \textbf{Diseño de arquitectura}
            \begin{itemize}
                  \item Se adoptó el patrón de diseño \textbf{Modelo–Vista–Controlador (MVC)}, mediante
                        la estructura de la Figura \ref{fig:mvc}. Esta arquitectura permitió separar
                        las responsabilidades del sistema, lo que buscó facilitar el mantenimiento y
                        escalabilidad.

                        \begin{figure}[H]
                              \centering
                              \includegraphics[width=0.7\textwidth]{assets/MVC.png}
                              \caption{Arquitectura MVC (Modelo, Vista, Controlador).}
                              \label{fig:mvc}
                        \end{figure}

                  \item \textbf{Modelos:}
                        \begin{itemize}
                              \item Representan entidades del sistema: usuarios, chats, mensajes, sesiones,
                                    documentos, categorías.
                              \item Cada modelo incluyó operaciones de creación, lectura, actualización y
                                    eliminación de registros (CRUD, por sus siglas en inglés) que se ejecutan
                                    directamente sobre la base de datos según sea requerido.
                              \item A través de la comunicación con los controladores, los modelos gestionan la
                                    persistencia y recuperación de datos de manera eficiente, siguiendo la lógica
                                    de negocio definida.
                        \end{itemize}
                  \item \textbf{Vistas:} En la implementación realizada del modelo MVC (Modelo-Vista-Controlador), las vistas corresponden a los puntos de conexión expuestos por el API, también llamados rutas o \textbf{\textit{endpoints}}. A través de la consulta a estos puntos de conexión, se consiguió acceso a las funciones definidas por el servidor; como registro de usuraios, inicio de sesión, listado de chats, envío de preguntas, entre otros.
                  \item \textbf{Controladores:} Su diseño buscó gestionar la lógica de negocio: validación de datos, comunicación con modelos,
                        manejo de errores y generación de respuestas. El Algoritmo \ref{alg:controlador-solicitudes} ilustra el flujo básico de un controlador típico.
                        \begin{algorithm}[H]
                              \caption{Controlador de solicitudes}
                              \label{alg:controlador-solicitudes}
                              \begin{algorithmic}[1]
                                    \Procedure{Controlador}{request}
                                    \State Validar(request.datos)
                                    \State resultado $\gets$ modelo.operacion(request)
                                    \State \textbf{devolver}(resultado)
                                    \EndProcedure
                              \end{algorithmic}
                        \end{algorithm}
                  \item \textbf{Módulos auxiliares:}
                        \begin{itemize}
                              \item \textbf{\textit{Middlewares} (Software intermedio):} fueron diseñadas como funciones ejecutadas antes de realizar la acción principal de cada punto de conexión. Se encargan de validar la autenticación y seguridad antes de pasar al controlador. El Algoritmo \ref{alg:validar-token} muestra un ejemplo de middleware para validar tokens JWT (JSON \textit{Web Token}.
                                    \begin{algorithm}[H]
                                          \caption{Validación de token}
                                          \label{alg:validar-token}
                                          \begin{algorithmic}[1]
                                                \Procedure{ValidarTokenRequest}{request}
                                                \If{not ValidarToken(request.token)}
                                                \State DevolverError(401, \guillemetleft{}Token inválido\guillemetright{})
                                                \Else
                                                \State Continuar(request)
                                                \EndIf
                                                \EndProcedure
                                          \end{algorithmic}
                                    \end{algorithm}
                              \item \textbf{Helpers (ayudantes):} funciones auxiliares reutilizadas a lo largo del código, como:
                                    \begin{itemize}
                                          \item encriptarContraseña(contraseña)
                                          \item generarToken(usuarioID)
                                          \item formatearFecha(fecha)
                                    \end{itemize}
                        \end{itemize}
            \end{itemize}

      \item \textbf{Diseño y construcción de bases de datos}
            \subsubsection{Base de datos relacional}
            Levantada en PostgreSQL, fue la encargada de almacenar la información
            estructurada del sistema; como usuarios, chats, mensajes y sesiones, de manera
            que se mantuvo la persistencia de datos y se facilitó la gestión de las
            interacciones del usuario con el asistente virtual. La Figura
            \ref{fig:diagrama-bd-relacional} muestra el diagrama entidad-relación (ER) de
            la base de datos relacional diseñada.

            \begin{figure}[H]
                  \centering
                  \includegraphics[width=0.9\textwidth]{assets/database\_uml.png}
                  \caption{Diagrama entidad-relación (ER) de la base de datos relacional.}
                  \label{fig:diagrama-bd-relacional}
            \end{figure}

            \begin{itemize}
                  \item Motor: PostgreSQL en AWS RDS (\textit{Relational Database Service}).
                  \item Entidades: usuarios, chats, mensajes, sesiones, documentos, categorías, códigos
                        de recuperación.
                  \item Relaciones:
                        \begin{itemize}
                              \item Un usuario puede tener varios chats.
                              \item Cada chat contiene múltiples mensajes.
                              \item Un usuario puede tener varias sesiones.
                              \item Un usuario puede subir varios documentos.
                              \item Una categoría puede incluir múltiples documentos.
                              \item Un usuario solo puede tener un único código de recuperación a la vez.
                        \end{itemize}
                  \item Mantenimiento: restricciones de claves foráneas, eliminación en cascada y
                        validaciones para asegurar la integridad referencial.
            \end{itemize}

            \subsubsection{Base de datos vectorial}
            Para llevar a cabo búsquedas semánticas eficientes y recuperar fragmentos de
            documentos relevantes en función de las preguntas del usuario (sistema de
            generación mejorada por recuperación), se implementó una base de datos
            vectorial utilizando Pinecone. Esta base almacena las representaciones
            numéricas generadas previamente a partir de los fragmentos de texto, lo que
            permitió generar consultas rápidas y precisas basadas en similitud semántica.

            \begin{itemize}
                  \item Motor: Pinecone.
                  \item Contenido: representaciones numéricas de fragmentos de documentos con metadatos
                        (fuente, categoría, documento, bloque, relevancia).
                  \item Obtención de los fragmentos más relevantes en relación al vector generado a
                        partir de la pregunta realizada. Esto lo realiza Pinecone de forma automática,
                        según el método de similitud configurado al momento de crear el índice. En este
                        caso, se indicó que se utilice la métrica de \textbf{similitud coseno}, puesto
                        que dicha métrica mide el ángulo de inclinación entre vectores, lo que
                        determina qué tan similar es la dirección a la que se dirigen (orientación
                        semántica). La fórmula que guía este cálculo es la siguiente:
                        \[\text{Similitud Coseno} (A, B) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}} \]
            \end{itemize}

            En esta etapa del desarrollo se definió un umbral de \textbf{0.4}, lo que
            indica que, para que una representación numérica devuelta por Pinecone sea
            tomada como válida, esta debe tener una coincidencia de, por lo menos, el 40\%
            con la representación numérica de la pregunta. Esta barrera puede variar a
            medida que se enriquece el corpus, ya que a mayor variedad hay más
            oportunidades de similitud entre vectores.

      \item \textbf{Implementación del modelo LLM y flujo RAG}

            Se implementó un flujo de generación mejorada por recuperación que permitió al
            asistente virtual generar respuestas fundamentadas en los contenidos educativos
            previamente indexados. Este flujo integró los siguientes componentes:

            \begin{enumerate}
                  \item \textbf{API en NodeJS:} Actúa como servidor principal al orquestar la
                        comunicación entre la aplicación móvil y los servicios de procesamiento
                        de lenguaje natural.
                  \item \textbf{Microservicio en Python:} Gestiona la interacción con el modelo
                        LLM de OpenAI para generar respuestas y procesar documentos.
                  \item \textbf{API de OpenAI:} Se utilizó para la generación de representaciones
                        numéricas (\textit{embeddings}) y respuestas contextualizadas.
                  \item \textbf{Base de datos vectorial (Pinecone):} Almacena y permite la
                        recuperación eficiente de las representaciones numéricas generadas a
                        partir de los contenidos educativos.
                  \item \textbf{Base de datos relacional (PostgreSQL):} Gestiona usuarios,
                        mensajes, sesiones e información de los documentos procesados, asegurando
                        la trazabilidad desde los vectores hasta el documento original.
            \end{enumerate}

            El flujo completo del proceso RAG, desde la recepción de una pregunta en la
            aplicación móvil hasta la devolución de la respuesta, se ilustra en la Figura
            \ref{fig:flujo-rag}.

            \begin{figure}[H]
                  \centering
                  \includegraphics[width=0.9\textwidth]{assets/RAG.png}
                  \caption{Flujo de procesamiento de preguntas mediante RAG.}
                  \label{fig:flujo-rag}
            \end{figure}

            \subsubsection{Arquitectura de almacenamiento distribuido}

            El almacenamiento de información a lo largo del sistema se distribuyó en tres
            elementos principales que trabajan de forma coordinada:

            \begin{itemize}
                  \item \textbf{Base de datos relacional (PostgreSQL):} se definió como el gestor principal de los
                        datos estructurados del sistema; almacena información de usuarios, chats,
                        mensajes, sesiones, códigos de recuperación de contraseñas y metadatos
                        básicos de los documentos procesados.
                  \item \textbf{Base de datos vectorial (Pinecone):} se designó como encargada de almacenar las
                        representaciones numéricas generadas a partir de los documentos
                        seleccionados. Constituye la base del funcionamiento RAG al permitir
                        recuperar el contexto necesario según la pregunta realizada, de manera que el
                        modelo LLM pueda generar respuestas fundamentadas.
                  \item \textbf{Almacenamiento de documentos originales (AWS S3):} implementado para almacenar todos
                        los documentos originales cargados al sistema. Esto permite realizar la consulta de estos documentos por parte usuarios con rol de \textbf{Administrador}.
            \end{itemize}

            Esta arquitectura distribuida buscó garantizar la trazabilidad completa desde
            cada vector indexado hasta su documento fuente, mientras mantuvo la eficiencia
            en las búsquedas semánticas y la integridad de los datos originales.

            \subsubsection{Flujo detallado de procesamiento de consultas}

            El microservicio de Python gestionó el flujo RAG completo al coordinar la
            interacción entre la base vectorial y el modelo grande de lenguaje, esto con el
            fin de generar respuestas contextualizadas y fundamentadas. El proceso se
            dividió en tres etapas principales:

            \begin{enumerate}
                  \item \textbf{Recepción de la pregunta en NodeJS:}
                        \begin{enumerate}
                              \item El usuario envía una pregunta en texto plano a través de la interfaz de la
                                    aplicación.
                              \item NodeJS recibe la pregunta y prepara la solicitud para el microservicio Python.
                        \end{enumerate}

                  \item \textbf{Procesamiento en Python:}
                        \begin{enumerate}
                              \item El microservicio Python recibe la pregunta enviada desde NodeJS.
                              \item Genera una representación numérica (\textit{embedding}) del texto de la
                                    pregunta utilizando el modelo \textit{text-embedding-3-small} de OpenAI,
                                    transformando la información textual en un vector semántico de 1536
                                    dimensiones.
                              \item Se realiza una consulta al índice de Pinecone con la representación numérica
                                    generada, recuperando los cinco fragmentos más relevantes del corpus
                                    vectorizado mediante similitud coseno, que servirán como contexto para la
                                    respuesta.
                              \item Si no se encuentran fragmentos relevantes, se procede a generar una respuesta
                                    estándar indicando la falta de información suficiente.
                              \item Si la búsqueda semántica obtuvo resultados, se extrae el texto plano almacenado
                                    en los metadatos de los vectores y se combina con la pregunta original,
                                    construyendo una petición que resume la información relevante y plantea la
                                    consulta al modelo grande de lenguaje.
                              \item Envía la petición al modelo LLM de OpenAI para generar la respuesta
                                    contextualizada.
                              \item Genera un objeto JSON que incluye la pregunta original, la respuesta obtenida y
                                    el tiempo de procesamiento, garantizando trazabilidad de la interacción.
                        \end{enumerate}

                  \item \textbf{Devolución de la respuesta a NodeJS:}
                        \begin{enumerate}
                              \item NodeJS recibe el JSON con la respuesta generada por el LLM.
                              \item Formatea y entrega la respuesta al usuario final a través de la interfaz de la
                                    aplicación.
                              \item Simultáneamente, guarda la pregunta y la respuesta en la base de datos
                                    relacional para mantener un historial completo de interacciones.
                        \end{enumerate}
            \end{enumerate}

            El flujo completo de procesamiento de preguntas se formaliza en el Algoritmo
            \ref{alg:flujo-completo-preguntas}.

            \begin{algorithm}[H]
                  \caption{Flujo completo de procesamiento de preguntas}
                  \label{alg:flujo-completo-preguntas}
                  \begin{algorithmic}[1]
                        \Procedure{ProcesarPregunta}{usuario}
                        \State pregunta $\gets$ RecibirPregunta(usuario)
                        \State EnviarPreguntaAPython(pregunta)
                        \State embedding $\gets$ GenerarEmbedding(pregunta)
                        \State contexto $\gets$ ConsultarPinecone(embedding, topK=5)
                        \State prompt $\gets$ ConstruirPrompt(pregunta, contexto)
                        \State respuesta $\gets$ ConsultarLLM(prompt)
                        \State jsonRespuesta $\gets$ ArmarJSON(pregunta, respuesta, tiempoProcesamiento)
                        \State EnviarAlCliente(jsonRespuesta)
                        \State GuardarMensaje(usuario, respuesta)
                        \EndProcedure
                  \end{algorithmic}
            \end{algorithm}
\end{enumerate}

\section{\textit{Sprint} 4: Desarrollo de la interfaz móvil en Kotlin}
\textbf{Duración:} 4 semanas

Este \textit{sprint} tuvo como objetivo diseñar e implementar la interfaz móvil
de la aplicación del asistente educativo, utilizando Kotlin para garantizar
integración nativa con Android y un flujo de interacción intuitivo para el
usuario. Esto permitió una interacción eficiente con el asistente al integrar
las funcionalidades proporcionadas por el servidor a través de servicios puntos
de conexión (\textit{endpoints}). El diseño de la arquitectura se basó en el
patrón de diseño MVVM (Modelo-Vista-Modelo de Vista), lo que permitió separar
responsabilidades, facilitar la escalabilidad del código y mantener una clara
independencia entre la lógica de negocio, la gestión de datos y la capa de
presentación.

\subsection{Ejecución}
\begin{enumerate}
      \item \textbf{Diseño de arquitectura}
            \begin{itemize}
                  \item Se adoptó el patrón de diseño \textbf{Modelo-Vista-Modelo de Vista (MVVM, por
                              sus siglas en inglés)} al seguir la estructura de la Figura \ref{fig:mvvm}.
                        Esta arquitectura permitió separar las responsabilidades del sistema, con el
                        fin de facilitar el mantenimiento y escalabilidad. Mediante este patrón de
                        diseño se buscó lograr una comunicación reactiva entre la interfaz de usuario y
                        las fuentes de datos, a través del uso de componentes de arquitectura de
                        Android como Datos Observables (\textit{LiveData}), Vista de modelo
                        (\textit{ViewModel}) y Repositorios (\textit{Repositories}).

                        \begin{figure}[H]
                              \centering
                              \includegraphics[width=0.7\textwidth]{assets/MVVM.png}
                              \caption{Arquitectura MVVM (Modelo, Vista, Modelo de Vista).}
                              \label{fig:mvvm}
                        \end{figure}

                  \item \textbf{Vista:}
                        \begin{itemize}
                              \item Se implementó a través de Actividades y Fragmentos organizados modularmente.
                              \item Cada fragmento representó una sección funcional del sistema (por ejemplo:
                                    inicio de sesión, chat, documentos, perfil).
                              \item Se aplicó el patrón de navegación basado en las librerías de Android
                                    \textit{NavHostFragment} y \textit{SafeArgs}, lo que busca garantizar
                                    transiciones seguras y controladas entre vistas.
                              \item Se usó componentes de diseño basados en \textit{Material Design 3}, el estandar
                                    de diseño de Google, para mantener consistencia visual, accesibilidad y
                                    adaptabilidad en distintos tamaños de pantalla
                                    \cite{material_design_m3_website}.
                              \item Cada Fragmento o Actividad contó con su propia plantilla, definida en formato
                                    XML (Lenguaje de Marcado Extensible, por sus siglas en inglés), que especifica
                                    la disposición de los elementos visuales.
                        \end{itemize}
                  \item \textbf{Modelo de Vista:}
                        \begin{itemize}
                              \item Fue definido como intermediario entre la vista y las fuentes de datos,
                                    encargado de manejar la lógica de presentación.
                              \item Se diseñó para emplear Datos Observables (\textit{LiveData}) y Flujos de Estado
                                    (\textit{StateFlow}) para notificar automáticamente a la vista sobre cambios en
                                    los datos.
                              \item Encapsuló la interacción con los repositorios para garantizar que la vista
                                    permanezca libre de lógica de negocio.
                              \item El Algoritmo \ref{alg:viewmodel-flujo} ilustra el flujo básico de un Modelo de
                                    Vista típico.

                                    \begin{algorithm}[H]
                                          \caption{Flujo básico de un Modelo de Vista}
                                          \label{alg:viewmodel-flujo}
                                          \begin{algorithmic}[1]
                                                \Procedure{ObtenerMensajes}{chatId}
                                                \State \textbf{emitir}(\texttt{Estado.Cargando})
                                                \State datos $\gets$ repositorio.obtenerMensajes(chatId)
                                                \State \textbf{emitir}(\texttt{Estado.Exitoso(datos)})
                                                \EndProcedure
                                          \end{algorithmic}
                                    \end{algorithm}

                        \end{itemize}
                  \item \textbf{Modelo (Repositorio y Fuentes de Datos):}
                        \begin{itemize}
                              \item Los repositorios centralizaron el acceso a las fuentes de datos, tanto locales
                                    como remotas.
                              \item Se definieron dos capas de origen de datos:
                                    \begin{enumerate}
                                          \item \textbf{Datos Locales:} implementados con la base de datos de Room, definido para gestionar entidades como usuarios, mensajes, chats y documentos. Esta capa permitió el acceso a información sin conexión a internet (historiales de chat, información del usuario, listado de documentos disponibles), así como el almacenamiento persistente de la información obtenida mediante otras fuentes de datos que sí requieren comunicación con internet.
                                          \item \textbf{Datos Remotos:} implementados mediante \textit{Retrofit} y \textit{OkHttp}, se utilizó para consumir los puntos de conexión (\textit{endpoints}) del servidor desarrollado en el \textit{sprint} anterior.
                                    \end{enumerate}
                              \item Los repositorios se encargan de determinar la fuente más apropiada según la
                                    disponibilidad de conexión y estado de sincronización.
                        \end{itemize}

                  \item \textbf{Gestión de dependencias:}
                        \begin{itemize}
                              \item Se empleó \textit{Hilt (Dagger)} para la inyección de dependencias, lo que
                                    simplificó la creación de instancias de Modelos de Vistas, repositorios y
                                    servicios. Este enfoque buscó garantizar el bajo acoplamiento entre componentes
                                    y favorecer la escalabilidad del sistema.
                              \item La configuración de los módulos de \textit{Hilt} se realizó en la carpeta
                                    \texttt{di/}, donde se definieron las dependencias necesarias para la
                                    aplicación; como clientes \textit{Retrofit}, bases de datos Room y
                                    repositorios.
                              \item Se utilizaron otras dependencias básicas como \textit{AppCompat} y
                                    \textit{ConstraintLayout} para asegurar compatibilidad y flexibilidad en el
                                    diseño de la interfaz, así como el Fragmento de Navegación
                                    (\textit{NavigationFragment}) para gestionar la navegación entre pantallas.
                              \item Por su parte, el uso de \textit{ThreeTen} permitió mantener la compatibilidad
                                    de manejo de fechas para versiones antiguas de Android (con soporte desde
                                    Android 7 en adelante), mientras que \textit{Glide} facilitó la carga y gestión
                                    eficiente de archivos dentro de la aplicación.
                        \end{itemize}
                  \item \textbf{Manejo de estado y persistencia:}
                        \begin{itemize}
                              \item Se usó el alcance del modelo de vista (\textit{ViewModelScope}) y el alcance de
                                    corrutina (\textit{CoroutineScope}) para ejecutar tareas asíncronas sin
                                    bloquear la interfaz.
                              \item Se implementó almacenamiento persistente mediante Preferencias Compartidas
                                    (\textit{SharedPreferences}) y Room, buscando que incluso sin internet la
                                    aplicación aún fuera utilizable, a pesar de no poder realizar peticiones al
                                    asistente, ya que esta función sí requiere comunicación con internet para
                                    realizar solicitudes a \textit{OpenAI}.
                              \item La base de datos local permitió al usuario acceder a su historial de chats,
                                    datos personales e historial de documentos (en el caso de usuarios con rol
                                    \textbf{Administrador}) aún sin conexión a internet.
                        \end{itemize}
            \end{itemize}

      \item \textbf{Integración con el servidor y servicios de Python}
            \begin{itemize}
                  \item Se verificó de forma directa el acceso del cliente al consumo de puntos de conexión del servidor para autenticación, gestión de
                        sesiones, envío de preguntas y recuperación de respuestas.
                  \item Se validó en la aplicación el procesamiento de respuestas JSON, conversión y renderizado de manera clara y comprensible.
                  \item Se definió el manejo de errores y reconexión ante fallos de red, lo que buscó asegurar robustez en la
                        experiencia de usuario.
            \end{itemize}
\end{enumerate}

\section{\textit{Sprint} 5: Pruebas y validación}
\textbf{Duración:} 3 semanas

Este \textit{sprint} se centró en validar el funcionamiento integral del
sistema para asegurar la correcta interacción entre la aplicación móvil, el
servidor, la base de datos relacional y vectorial y el modelo de lenguaje (LLM,
por sus siglas en inglés). Además, se buscó evaluar la calidad, precisión y
confiabilidad de las respuestas generadas por el asistente virtual mediante la
metodología y métricas propuestas en las secciones 6.3.9 y 6.3.10 de este
documento.

\subsection{Ejecución}

\begin{enumerate}
      \item \textbf{Pruebas funcionales del sistema}

            Las pruebas funcionales se llevaron a cabo al seguir un proceso estructurado
            compuesto por cinco fases: configuración del entorno, verificación de
            comunicación entre módulos, pruebas de puntos de conexión, verificación de
            operaciones CRUD y validación de integridad entre fuentes de datos.

            \begin{itemize}
                  \item \textbf{1. Configuración del entorno de prueba.}
                        Se desplegó el servidor en un entorno local sobre NodeJS mediante el uso de las mismas variables de entorno del entorno de producción. La base de datos relacional (PostgreSQL) se inicializó con la estructura definitiva del proyecto y una carga mínima de datos para pruebas. En el caso de la base de datos vectorial, Pinecone se configuró con el índice previamente alimentado; mientras que al contenedor S3 ya habían sido cargados los documentos del corpus inicial.

                        Para que la aplicación móvil tuviera acceso al servidor local, el equipo en el
                        que se levantó el servidor fue habilitado como punto de conexión inalámbrico.
                        De esta manera, el dispositivo Android de pruebas pudo conectarse a la misma
                        red y utilizar la dirección IP de la computadora como la IP del servidor.

                  \item \textbf{2. Verificación de comunicación entre módulos.}
                        Para asegurar que cada componente podía comunicarse con el siguiente, se realizaron solicitudes controladas del tipo \textbf{\guillemetleft{}ping\guillemetright{}}.
                        \begin{itemize}
                              \item \textbf{Desde la aplicación móvil al servidor:} envío de solicitudes HTTP simulando
                                    inicio de sesión y envío de mensajes.
                              \item \textbf{Del servidor a la base de datos relacional:} ejecución de consultas directas de lectura y
                                    escritura.
                              \item \textbf{Del servidor a la base de datos vectorial:} inserción y recuperación de vectores de prueba.
                              \item \textbf{Del servidor al contenedor S3:} subida, lectura y eliminación de documentos.
                        \end{itemize}
                        Se consideró exitosa la comunicación cuando cada módulo pudo responder con el código de estado esperado y en el formato establecido.

                  \item \textbf{3. Pruebas de puntos de conexión mediante Postman.}
                        Se creó una colección de Postman con los puntos de conexión del sistema, agrupados por funcionalidad:
                        \texttt{/autenticación}, \texttt{/mensajes}, \texttt{/historial}, \texttt{/documentos}.
                        Para cada punto de conexión se diseñaron casos de prueba que contemplaron:
                        \begin{itemize}
                              \item Parámetros válidos e inválidos.
                              \item Códigos de estado esperados.
                              \item Verificación de restricciones de acceso (por rol o por token).
                              \item Validación de formatos de respuesta (JSON).
                              \item Manejo de errores (HTTP 400 y HTTP 500).
                        \end{itemize}
                        La prueba se consideró exitosa si la respuesta coincidía con el comportamiento esperado para cada punto de conexión definido en el servidor.

                  \item \textbf{4. Pruebas CRUD en las bases de datos.}
                        Se verificaron las operaciones de creación, lectura, actualización y eliminación tanto en PostgreSQL como en Pinecone.
                        Por cada operación CRUD realizada desde el cliente, se confirmó:
                        \begin{itemize}
                              \item Que la operación se reflejara correctamente en la base de datos relacional.
                              \item Que el vector correspondiente se actualizara o eliminara en Pinecone.
                              \item Que en caso de involucrar documentos, los cambios se reflejaran también en el
                                    contenedor S3.
                        \end{itemize}
                        Se revisó manualmente cada operación mediante consultas SQL directas y mediante la consola de Pinecone.

                  \item \textbf{5. Validación de integridad entre fuentes de datos.}
                        Para comprobar la consistencia entre las tres fuentes de datos (PostgreSQL, Pinecone y contenedor S3), se ejecutaron casos en los que:
                        \begin{itemize}
                              \item \textbf{Se creaba un documento desde la aplicación:} En el resultado esperado, este documento debía aparecer en la base
                                    relacional, almacenarse en S3 y generarse su vector en Pinecone.
                              \item \textbf{Se eliminaba un documento} En el resultado esperado, el documento debía eliminarse de los tres sistemas.
                        \end{itemize}
                        Se consideró exitosa la prueba cuando los cambios se replicaban de forma consistente en todas las fuentes, sin incongruencias.
            \end{itemize}

      \item \textbf{Pruebas de calidad y confiabilidad de las respuestas}

            \begin{itemize}
                  \item \textbf{1. Construcción del conjunto de pruebas.}
                        Se elaboró una lista de 55 preguntas:
                        \begin{itemize}
                              \item 45 relacionadas con el corpus, extraídas con herramientas comerciales de Inteligencia Artificial, como \textbf{NotebookLLM}.
                              \item 10 preguntas de control, no asociadas al corpus, para evaluar la \textbf{no alucinación}.
                        \end{itemize}
                        Cada pregunta fue almacenada en una hoja de cálculo, junto con las referencias que se esperaba que el modelo consultara para responder cada pregunta.

                  \item \textbf{2. Ejecución manual de cada consulta.}
                        Se envió cada pregunta al sistema desde la aplicación móvil, asegurando que:
                        \begin{itemize}
                              \item el historial de chat estuviera limpio antes de cada prueba,
                              \item la pregunta se procesara en un nuevo mensaje,
                              \item se registraran:
                                    \begin{itemize}
                                          \item el tiempo de respuesta,
                                          \item la respuesta generada,
                                          \item las referencias devueltas por el sistema.
                                    \end{itemize}
                        \end{itemize}
                        Para calcular el tiempo de respuesta, el sistema inicia un contador justo antes de realizar la llamada al servicio de Python, y lo detiene hasta que se devuelve la respuesta final al cliente.

                  \item \textbf{3. Validación de alineación con el corpus.}
                        Cada respuesta obtenida por el sistema, fue clasificada como:
                        \begin{itemize}
                              \item \textbf{Exitosa:} si el sistema fue capaz de responderla.
                              \item \textbf{Congruente:} si el comportamiento esperado se cumplió. Si la pregunta era de control, el comportamiento esperado fue que el sistema no pudiera responder.
                        \end{itemize}
                        Si el modelo respondía cuando no debía hacerlo, o viceversa, se consideraba comoo \textbf{incongruente}

                  \item \textbf{4. Verificación de referencias.}
                        Para cada respuesta obtenida del sistema, se compararon las referencias consultadas contra las referencias esperadas, catalogando cada coincidencia como:
                        \begin{itemize}
                              \item \textbf{Exacta:} si las referencias esperadas y las consultadas por el modelo coinciden por completo.
                              \item\textbf{Parcial:} si las referencias consultadas incluyen algunas de las referencias esperadas, pero también el modelo hizo uso de referencias adicionales.
                              \item \textbf{Nula:} si las referencias utilizadas difieren completamente de las referencias esperadas.
                        \end{itemize}

                  \item \textbf{5. Medición de latencia.}
                        Para cada una de las 55 consultas se registró el tiempo exacto entre solicitud y respuesta. Posteriormente, se calculó:
                        \begin{itemize}
                              \item tiempo mínimo,
                              \item tiempo máximo,
                              \item tiempo promedio,
                              \item desviación estándar entre tiempos.
                        \end{itemize}
                        Este proceso permite que cualquier evaluador pueda reproducir la medición bajo las mismas condiciones controladas.
            \end{itemize}
\end{enumerate}

\section{\textit{Sprint} 6: Documentación y presentación}
\textbf{Duración:} 3 semanas

Este \textit{sprint} se centró en consolidar toda la documentación generada
durante el desarrollo del proyecto y preparar la presentación final del
asistente virtual de formación ciudadana y valores morales. El objetivo fue
garantizar que tanto los resultados como los procesos utilizados quedaran
claramente registrados, así como asegurar que el producto final estuviera
disponible para revisión, prueba y entrega formal al cliente.

\subsection{Ejecución}
\begin{enumerate}
      \item \textbf{Se elaboró el informe final por medio de}
            \begin{itemize}
                  \item La integración de la información de todos los \textit{sprints} previos en un
                        documento único, estructurado y coherente.
                  \item La inclusión de resultados de cada \textit{sprint}, análisis de hallazgos,
                        decisiones de diseño y mejoras implementadas.
                  \item La redacción de conclusiones generales y recomendaciones para futuras
                        iteraciones, escalabilidad o mejoras del asistente virtual.
                  \item El formateo del documento en LaTeX, asegurando uniformidad, claridad y
                        cumplimiento de estándares académicos y de presentación profesional.
            \end{itemize}

      \item \textbf{Se preparó la presentación final, que incluyó}
            \begin{itemize}
                  \item El desarrollo del material visual que resumió el proyecto, así como diagramas
                        de arquitectura, capturas de pantalla del prototipo móvil, flujo de interacción
                        y ejemplos de uso del asistente.
                  \item La elaboración de una presentación estructurada para explicar el proceso de
                        desarrollo, resultados obtenidos y funcionalidades del sistema.
                  \item El ensayo de la presentación y ajuste de contenido para garantizar claridad,
                        concisión y relevancia para el público evaluador.
            \end{itemize}

      \item \textbf{Se trasladó y entregó toda la documentación al cliente, a través de}
            \begin{itemize}
                  \item La consolidación de repositorios de código, documentos de investigación, fichas
                        de usuario, diagramas de arquitectura y demás materiales generados.
                  \item La entrega formal de toda la información consolidada a la Fundación de
                        Scouts de Guatemala, con el objetivo de asegurar el acceso a todos los recursos para
                        pruebas, mantenimiento y futuras actualizaciones.
                  \item El registro de la entrega, que incluye el inventario de archivos, la versión final de toda la
                        documentación generada y la evidencia de disponibilidad del producto para pruebas finales.
            \end{itemize}
\end{enumerate}